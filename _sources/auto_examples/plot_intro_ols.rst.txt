
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_intro_ols.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_intro_ols.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_intro_ols.py:


Comparison of CD, GD, inertial and Anderson acceleration
========================================================

Coordinate descent outperforms gradient descent, and Anderson acceleration
outperforms inertial acceleration.

.. GENERATED FROM PYTHON SOURCE LINES 8-21

.. code-block:: default

    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt
    from numpy.linalg import norm
    from scipy.sparse.linalg import cg
    from libsvmdata import fetch_libsvm
    from andersoncd.plot_utils import configure_plt, _plot_legend_apart

    from andersoncd.lasso import solver_enet, primal_enet, apcg_enet


    configure_plt()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    usetex mode requires TeX.




.. GENERATED FROM PYTHON SOURCE LINES 22-23

Load the data:

.. GENERATED FROM PYTHON SOURCE LINES 23-46

.. code-block:: default


    n_features = 1000
    X, y = fetch_libsvm('rcv1_train', normalize=True)
    X = X[:, :n_features]

    y -= y.mean()
    y /= norm(y)

    # conjugate gradient competitor:
    E_cg = []
    E_cg.append(norm(y) ** 2 / 2)


    def callback(x):
        pobj = primal_enet(y - X @ x, x, 0)
        E_cg.append(pobj)


    w_star = cg(
        X.T @ X, X.T @ y, callback=callback, maxiter=500, tol=1e-32)[0]
    E_cg = np.array(E_cg)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Dataset: rcv1_train




.. GENERATED FROM PYTHON SOURCE LINES 47-48

Run algorithms:

.. GENERATED FROM PYTHON SOURCE LINES 48-100

.. code-block:: default


    # solvers parameters:
    alpha = 0  # Least Squares
    tol = 1e-13
    max_iter = 2000
    f_gap = 10

    all_algos = [
        ('pgd', False),
        ('cd', False),
        ('pgd', True),
        ('cd', True),
        ('fista', False),
        ('apcg', False)
    ]

    dict_algo_name = {}
    dict_algo_name["pgd", False] = "GD"
    dict_algo_name["cd", False] = "CD"
    dict_algo_name["pgd", True] = "GD - Anderson"
    dict_algo_name["cd", True] = "CD - Anderson"
    dict_algo_name["fista", False] = "GD - inertial"
    dict_algo_name["apcg", False] = "CD - inertial"


    dict_Es = {}

    for algo in all_algos:
        print("Running ", dict_algo_name[algo])
        if algo[0] == 'apcg':
            w, E, gaps = apcg_enet(
                X, y, alpha, max_iter=max_iter, tol=tol, f_gap=f_gap,
                verbose=False)
        else:
            w, E, _ = solver_enet(
                X, y, alpha=alpha, f_gap=f_gap, max_iter=max_iter, tol=tol,
                algo=algo[0], use_acc=algo[1], verbose=False)
        dict_Es[algo] = E.copy()


    current_palette = sns.color_palette("colorblind")
    dict_color = {}
    dict_color["pgd"] = current_palette[0]
    dict_color["fista"] = current_palette[0]
    dict_color["cd"] = current_palette[1]
    dict_color["apcg"] = current_palette[1]


    p_star = primal_enet(y - X @ w_star, w_star, alpha)
    for E in dict_Es.values():
        p_star = min(p_star, min(E))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Running  GD
    Running  CD
    Running  GD - Anderson
    Running  CD - Anderson
    Running  GD - inertial
    Running  CD - inertial




.. GENERATED FROM PYTHON SOURCE LINES 101-102

Plot convergence curves:

.. GENERATED FROM PYTHON SOURCE LINES 102-154

.. code-block:: default


    save_fig = False
    # save_fig = True
    if save_fig:
        figsize = (8, 3)
    else:
        figsize = (10, 5)

    plt.close('all')
    fig, ax = plt.subplots(figsize=figsize)


    for i, algo in enumerate(all_algos):
        E = dict_Es[algo]
        use_acc = algo[1]
        if use_acc:
            linestyle = 'dashed'
        elif algo[0].startswith(('fista', 'apcg')):
            linestyle = 'dotted'
        else:
            linestyle = 'solid'

        if i == 2:
            ax.semilogy(
                np.arange(len(E_cg)), E_cg - p_star, label="conjugate grad.",
                color='black', linestyle='dashdot')
        ax.semilogy(
            f_gap * np.arange(len(E)), E - p_star,
            label=dict_algo_name[algo],
            color=dict_color[algo[0]], linestyle=linestyle)


    plt.ylabel(r"$f(x^{(k)}) - f(x^{*})$")
    plt.xlabel(r"iteration $k$")
    ax.set_yticks((1e-15, 1e-10, 1e-5, 1e0))
    plt.tight_layout()


    fig_dir = "../"
    fig_dir_svg = "../"

    if save_fig:
        fig.savefig(
            "%sintro_ols.pdf" % fig_dir, bbox_inches="tight")
        fig.savefig(
            "%sintro_ols.svg" % fig_dir_svg, bbox_inches="tight")
        fig = _plot_legend_apart(
            ax, "%sintro_ols_legend.pdf" % fig_dir, ncol=3)

    plt.title("Convergence on Least Squares")
    plt.legend()
    plt.show(block=False)



.. image:: /auto_examples/images/sphx_glr_plot_intro_ols_001.png
    :alt: Convergence on Least Squares
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  21.545 seconds)


.. _sphx_glr_download_auto_examples_plot_intro_ols.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_intro_ols.py <plot_intro_ols.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_intro_ols.ipynb <plot_intro_ols.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
